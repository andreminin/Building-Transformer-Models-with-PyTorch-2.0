{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee566cd5",
   "metadata": {},
   "source": [
    "# Implementing Transformer with GPT-Style LM on IMDB with PyTorch\n",
    "\n",
    "This notebook trains a small GPT-style Transformer (encoder-only with causal mask) on the IMDB dataset for language modeling.\n",
    "- Tokenizer: **GPT-2** (pad token set to EOS)\n",
    "- Objective: next-token prediction (causal LM)\n",
    "- Includes: training, validation (loss/perplexity), checkpointing, and single/batch text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39005217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2525acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<|endoftext|>', '<|endoftext|>', 50257)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load IMDB (25k train / 25k test)\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Load GPT-2 tokenizer; set PAD to EOS for convenience\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.pad_token, tokenizer.eos_token, tokenizer.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03ae474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000, torch.Size([256]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# To keep training light in this demo, you can reduce max_length to 256 (or keep 512 for longer context).\n",
    "MAX_LEN = 256\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "# Remove columns we don't need after tokenization\n",
    "train_ds = dataset[\"train\"].map(tokenize_fn, batched=True, remove_columns=[\"text\", \"label\"])\n",
    "val_ds   = dataset[\"test\"].map(tokenize_fn,  batched=True, remove_columns=[\"text\", \"label\"])\n",
    "\n",
    "# Make PyTorch-friendly\n",
    "train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "val_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "len(train_ds), len(val_ds), train_ds[0][\"input_ids\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "747b15c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "def collate(batch):\n",
    "    input_ids = torch.stack([b[\"input_ids\"] for b in batch], dim=0)      # [B, L]\n",
    "    attention_mask = torch.stack([b[\"attention_mask\"] for b in batch],0) # [B, L]\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
    "\n",
    "next(iter(train_loader))[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa931d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000, batch_first: bool = True):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B, L, D] if batch_first else [L, B, D]\n",
    "        if self.batch_first:\n",
    "            x = x + self.pe[:, :x.size(1), :]\n",
    "        else:\n",
    "            x = x + self.pe[:, :x.size(0), :].transpose(0, 1)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class GPTStyleDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=512, num_layers=6, nhead=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, dropout, batch_first=True)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, nhead=nhead, dim_feedforward=2048, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.d_model = embedding_dim\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # input_ids: [B, L], attention_mask: [B, L] (1=token, 0=pad)\n",
    "        x = self.embedding(input_ids) * (self.d_model ** 0.5)   # [B, L, D]\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        L = x.size(1)\n",
    "        # Causal mask: [L, L]\n",
    "        causal_mask = torch.triu(torch.ones(L, L, device=x.device), diagonal=1).bool()\n",
    "        # src_key_padding_mask: True where we want to mask (i.e., PAD)\n",
    "        src_key_padding_mask = (attention_mask == 0) if attention_mask is not None else None\n",
    "\n",
    "        x = self.transformer(\n",
    "            x,\n",
    "            mask=causal_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )  # [B, L, D]\n",
    "\n",
    "        logits = self.fc(x)  # [B, L, V]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27632e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.300945"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "EMBED_DIM = 768\n",
    "LAYERS = 4\n",
    "NHEAD = 8\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = GPTStyleDecoder(vocab_size=tokenizer.vocab_size, embedding_dim=EMBED_DIM, num_layers=LAYERS, nhead=NHEAD, dropout=DROPOUT).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "sum(p.numel() for p in model.parameters())/1e6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "998bdcd4-dcf5-4c9a-8a4c-fc68c42f0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "\n",
    "def cleanup_cuda():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "cleanup_cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d23158d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6250/6250 [06:50<00:00, 15.24it/s, avg_loss=5.6, loss=5.11]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6250/6250 [01:32<00:00, 67.85it/s, avg_loss=5.28, loss=5.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | Train loss 5.6047, ppl 271.70 | Val loss 5.2768, ppl 195.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6250/6250 [06:49<00:00, 15.25it/s, avg_loss=5.03, loss=5.16]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6250/6250 [01:32<00:00, 67.88it/s, avg_loss=5.05, loss=5.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 | Train loss 5.0273, ppl 152.52 | Val loss 5.0541, ppl 156.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6250/6250 [06:49<00:00, 15.25it/s, avg_loss=4.78, loss=4.71]\n",
      "Validation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6250/6250 [01:32<00:00, 67.93it/s, avg_loss=4.96, loss=4.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 | Train loss 4.7805, ppl 119.16 | Val loss 4.9591, ppl 142.47\n",
      "Saved to checkpoints/gptstyle_imdb.pt\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train(train)\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Create progress bar\n",
    "    pbar = tqdm(loader, desc=\"Training\" if train else \"Validation\")\n",
    "    \n",
    "    for input_ids, attention_mask in pbar:  # Changed from loader to pbar\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask=attention_mask)  # [B, L, V]\n",
    "\n",
    "        # Shift for next-token prediction\n",
    "        shift_logits = logits[:, :-1, :].contiguous().view(-1, tokenizer.vocab_size)   # [(B*(L-1)), V]\n",
    "        shift_labels = input_ids[:, 1:].contiguous().view(-1)                           # [(B*(L-1))]\n",
    "\n",
    "        loss = criterion(shift_logits, shift_labels)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Count non-pad target tokens for averaging\n",
    "        not_pad = (shift_labels != tokenizer.pad_token_id).sum().item()\n",
    "        total_loss += loss.item() * not_pad\n",
    "        total_tokens += not_pad\n",
    "\n",
    "        avg_loss = total_loss / max(total_tokens, 1)\n",
    "        pbar.set_postfix(loss=loss.item(), avg_loss=avg_loss)\n",
    "\n",
    "    pbar.close()  # Close progress bar to avoid memory leaks\n",
    "    \n",
    "    avg_loss = total_loss / max(total_tokens, 1)\n",
    "    ppl = math.exp(avg_loss) if avg_loss < 20 else float(\"inf\")\n",
    "    return avg_loss, ppl\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_ppl = run_epoch(train_loader, train=True)\n",
    "    val_loss, val_ppl = run_epoch(val_loader, train=False)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train loss {train_loss:.4f}, ppl {train_ppl:.2f} | Val loss {val_loss:.4f}, ppl {val_ppl:.2f}\")\n",
    "\n",
    "# Save final model\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"checkpoints/gptstyle_imdb.pt\")\n",
    "print(\"Saved to checkpoints/gptstyle_imdb.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84981893",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int | None = None,\n",
    "    device: str = \"cpu\"\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    # Encode\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    if len(input_ids) == 0:\n",
    "        input_ids = [tokenizer.eos_token_id]\n",
    "    generated = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)  # [1, L]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        L = generated.size(1)\n",
    "        causal_mask = torch.triu(torch.ones(L, L, device=device), diagonal=1).bool()\n",
    "        attn_mask = torch.ones_like(generated, device=device)  # no pads\n",
    "        logits = model(generated, attention_mask=attn_mask)  # [1, L, V]\n",
    "        next_logits = logits[:, -1, :]  # [1, V]\n",
    "\n",
    "        if temperature <= 0:\n",
    "            next_token = torch.argmax(next_logits, dim=-1)\n",
    "        else:\n",
    "            next_logits = next_logits / temperature\n",
    "            if top_k is not None:\n",
    "                v, ix = torch.topk(next_logits, k=top_k, dim=-1)\n",
    "                filtered = torch.full_like(next_logits, -float(\"inf\"))\n",
    "                filtered.scatter_(1, ix, v)\n",
    "                next_logits = filtered\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).squeeze(1)  # [1]\n",
    "\n",
    "        generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "    return tokenizer.decode(generated[0].tolist(), skip_special_tokens=True)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_batch(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompts: list[str],\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int | None = None,\n",
    "    device: str = \"cpu\"\n",
    ") -> list[str]:\n",
    "    model.eval()\n",
    "    # Encode & left-pad to same length (we'll use pad_token_id)\n",
    "    enc = [tokenizer.encode(p, add_special_tokens=False) for p in prompts]\n",
    "    enc = [e if len(e) > 0 else [tokenizer.eos_token_id] for e in enc]\n",
    "    max_len = max(len(e) for e in enc)\n",
    "\n",
    "    B = len(enc)\n",
    "    generated = torch.full((B, max_len), tokenizer.pad_token_id, dtype=torch.long, device=device)\n",
    "    for i, e in enumerate(enc):\n",
    "        generated[i, :len(e)] = torch.tensor(e, device=device)\n",
    "\n",
    "    # We will treat initial pads as real tokens in attention_mask=1 for simplicity,\n",
    "    # but a cleaner approach is to keep a true padding mask. We'll build a proper mask.\n",
    "    attn_mask = (generated != tokenizer.pad_token_id).long()  # [B, L]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        L = generated.size(1)\n",
    "        causal_mask = torch.triu(torch.ones(L, L, device=device), diagonal=1).bool()\n",
    "        logits = model(generated, attention_mask=attn_mask)  # [B, L, V]\n",
    "        next_logits = logits[:, -1, :]  # [B, V]\n",
    "\n",
    "        if temperature <= 0:\n",
    "            next_token = torch.argmax(next_logits, dim=-1)  # [B]\n",
    "        else:\n",
    "            next_logits = next_logits / temperature\n",
    "            if top_k is not None:\n",
    "                v, ix = torch.topk(next_logits, k=top_k, dim=-1)\n",
    "                filtered = torch.full_like(next_logits, -float(\"inf\"))\n",
    "                filtered.scatter_(1, ix, v)\n",
    "                next_logits = filtered\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).squeeze(1)  # [B]\n",
    "\n",
    "        # Append new step to all sequences\n",
    "        generated = torch.cat([generated, next_token.unsqueeze(1)], dim=1)\n",
    "        # Update attention mask (new token is non-pad)\n",
    "        attn_mask = torch.cat([attn_mask, torch.ones(B, 1, dtype=torch.long, device=device)], dim=1)\n",
    "\n",
    "    # Decode each\n",
    "    outputs = []\n",
    "    for i in range(B):\n",
    "        outputs.append(tokenizer.decode(generated[i].tolist(), skip_special_tokens=True))\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f80d633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, I was a kid in the theater. I was so impressed by the movie, and it was just a good movie. I was really looking forward to seeing it. I was expecting a movie with\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load checkpoint (if re-running from scratch, comment out if model not yet trained)\n",
    "state = torch.load(\"checkpoints/gptstyle_imdb.pt\", map_location=device)\n",
    "model.load_state_dict(state)\n",
    "\n",
    "print(generate(model, tokenizer, \"Once upon a time\", max_new_tokens=40, temperature=0.2, top_k=50, device=device))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c249775-03a0-453b-b299-10317d28467b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: The T-1000 character is\n",
      "The T-1000 character is, the whole movie is even better than it. the other movie would be about. I give it the 1. The whole movie a 4. It's a shame that it never got better. It\n",
      "\n",
      "Prompt: In a small town, there was\n",
      "In a small town, there was a lot of things that went wrong.<br /><br />I am very glad that this is something that is in a way I can say about this film. It has a great message of the\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"The future of AI is\", \"In a small town, there was\"]\n",
    "outs = generate_batch(model, tokenizer, prompts, max_new_tokens=40, temperature=0.8, top_k=50, device=device)\n",
    "for p, o in zip(prompts, outs):\n",
    "    print(f\"\\nPrompt: {p}\\n{o}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf9f55-34e5-4228-86d4-e9e802ac0572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
