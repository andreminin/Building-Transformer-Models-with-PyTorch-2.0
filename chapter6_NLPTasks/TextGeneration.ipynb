{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.11 (main, Jul 23 2025, 00:34:44) [Clang 20.1.4 ]\n",
      "PyTorch: 2.8.0+cu129\n",
      "Transformers: 4.56.0\n",
      "Loaded utils from: /mnt/nfs/workspace/courses/PyTorch/Building-Transformer-Models-with-PyTorch-2.0/utils.py\n",
      "Memory environment configured\n",
      "Selected device: cuda\n",
      "GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import platform\n",
    "\n",
    "# Set environment variable to help with memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Environment & version checks\n",
    "# Import CUDA utils from parent folder (preferred), fallback to local\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "try:\n",
    "    import torch, transformers\n",
    "    print(\"PyTorch:\", torch.__version__)\n",
    "    print(\"Transformers:\", transformers.__version__)\n",
    "except Exception as e:\n",
    "    print(\"You likely need to install torch/transformers:\", e)\n",
    "    \n",
    "# Try parent directory first (ideal location)\n",
    "parent_dir = str(Path.cwd().parent)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "try:\n",
    "    import utils  # expected at ../utils.py\n",
    "except Exception:\n",
    "    # Fallback: current working directory\n",
    "    curr_dir = str(Path.cwd())\n",
    "    if curr_dir not in sys.path:\n",
    "        sys.path.insert(0, curr_dir)\n",
    "    import utils  # tries ./utils.py\n",
    "\n",
    "from utils import *\n",
    "\n",
    "print(\"Loaded utils from:\", utils.__file__)\n",
    "# Set memory env & show current device\n",
    "utils.setup_memory_environment(expandable_segments=True)\n",
    "device = utils.get_device()\n",
    "print(\"Selected device:\", device)\n",
    "\n",
    "full_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "referenced_widgets": [
      "1a3ccd8dadf3402d896bf0c9dc04fa47",
      "39c9a3356b68455dba516ec32053b3bb",
      "f9d8fd50d9734aa6abc920af828b4c6d",
      "879b7ae4ec0f45879f90e3f769d67896",
      "22caa164d8c2473bbb999707093aa2af",
      "db2b6b6aab394ab5a4aaf1cb74f129ca",
      "fa782cb66f5d431fad49ac8ecae17c80",
      "8e4e2d11cf814f8bb12331df508cca9e",
      "54c0e98097654319a7fec8b752c0d717",
      "076b12597eab4c59b33ad5b53803311e",
      "dad79375e4944b2989b7791800a6a946"
     ]
    },
    "id": "Pv56f7UaYWAw",
    "outputId": "da0fc5c5-1f2e-428e-ed3e-326c8ab1ca21"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    " \n",
    "# TinyLlama - 1.1B parameters (small and fast)\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "# 4-bit quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id  # Use EOS token as pad token\n",
    "tokenizer.padding_side = \"left\"\n",
    " \n",
    "# Load the dataset\n",
    "#dataset = load_dataset(\"tiny_shakespeare\", revision=\"main\")\n",
    "# Load from the specific GitHub URL\n",
    "dataset = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\"train\": \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"}\n",
    ")\n",
    "\n",
    "'''\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['text'],\n",
    "        num_rows: 1\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['text'],\n",
    "        num_rows: 1\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['text'],\n",
    "        num_rows: 1\n",
    "    })\n",
    "})\n",
    "'''\n",
    "\n",
    "# Split the continuous text into smaller chunks\n",
    "def split_text(text, max_length=100):\n",
    "    return [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "# Apply the split_text function to the dataset\n",
    "\n",
    "\n",
    "split_texts = split_text(dataset[\"train\"][\"text\"][0])\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the split_texts\n",
    "tokenized_texts = tokenizer(split_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n",
    "\n",
    "class ShiftedDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.encodings[\"input_ids\"][idx]\n",
    "        attention_mask = self.encodings[\"attention_mask\"][idx]\n",
    "        labels = input_ids[1:].tolist() + [tokenizer.eos_token_id]\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": torch.tensor(labels)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "# Create a DataLoader\n",
    "train_dataset = ShiftedDataset(tokenized_texts)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kLjkG6s2YjZd",
    "outputId": "8a607c9c-c8a9-4b3b-9b6e-a902365fc499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  3824, 21353, 19642, 29901]])\n",
      "tensor([[1, 1, 1, 1, 1]])\n",
      "tensor([[ 3824, 21353, 19642, 29901,     2]])\n"
     ]
    }
   ],
   "source": [
    "item=next(iter(train_dataloader))\n",
    "print(item['input_ids'])\n",
    "print(item['attention_mask'])\n",
    "print(item['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FWMR85OQYtdS"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Initialize the Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Configure the training arguments\n",
    "num_epochs = 20\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Initialize the GPT-2 model and optimizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Prepare the model and optimizer for training with Accelerator\n",
    "model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BvY6jfFIlv_E"
   },
   "outputs": [],
   "source": [
    "num_epochs=40\n",
    "epoch=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKdAzfSRgMKu",
    "outputId": "63ce3a8d-d0db-4be4-e456-d72b86a09b25"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40:   0%|                                                                                                                                                                | 0/1 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "Epoch 1/40: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.77it/s, Loss=14.4406, Avg Loss=14.4406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 - Average Loss: 14.4406\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.90it/s, Loss=12.9112, Avg Loss=12.9112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 - Average Loss: 12.9112\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.29it/s, Loss=10.0546, Avg Loss=10.0546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 - Average Loss: 10.0546\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.50it/s, Loss=9.0267, Avg Loss=9.0267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 - Average Loss: 9.0267\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.43it/s, Loss=6.4970, Avg Loss=6.4970]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 - Average Loss: 6.4970\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.40it/s, Loss=7.4582, Avg Loss=7.4582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 - Average Loss: 7.4582\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.48it/s, Loss=6.9754, Avg Loss=6.9754]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 - Average Loss: 6.9754\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.25it/s, Loss=6.2760, Avg Loss=6.2760]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 - Average Loss: 6.2760\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.54it/s, Loss=2.0550, Avg Loss=2.0550]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 - Average Loss: 2.0550\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.28it/s, Loss=2.2235, Avg Loss=2.2235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 - Average Loss: 2.2235\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.55it/s, Loss=0.4841, Avg Loss=0.4841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 - Average Loss: 0.4841\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.43it/s, Loss=0.1839, Avg Loss=0.1839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 - Average Loss: 0.1839\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.41it/s, Loss=0.1890, Avg Loss=0.1890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 - Average Loss: 0.1890\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.35it/s, Loss=0.0141, Avg Loss=0.0141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 - Average Loss: 0.0141\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.38it/s, Loss=0.0247, Avg Loss=0.0247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 - Average Loss: 0.0247\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.41it/s, Loss=0.0098, Avg Loss=0.0098]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 - Average Loss: 0.0098\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.37it/s, Loss=0.0174, Avg Loss=0.0174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 - Average Loss: 0.0174\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.38it/s, Loss=0.0020, Avg Loss=0.0020]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 - Average Loss: 0.0020\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.61it/s, Loss=0.0013, Avg Loss=0.0013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 - Average Loss: 0.0013\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.45it/s, Loss=2.6749, Avg Loss=2.6749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 - Average Loss: 2.6749\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.51it/s, Loss=0.0212, Avg Loss=0.0212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 - Average Loss: 0.0212\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.45it/s, Loss=0.0014, Avg Loss=0.0014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 - Average Loss: 0.0014\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.45it/s, Loss=0.0018, Avg Loss=0.0018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 - Average Loss: 0.0018\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.26it/s, Loss=0.0007, Avg Loss=0.0007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 - Average Loss: 0.0007\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.45it/s, Loss=1.6958, Avg Loss=1.6958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 - Average Loss: 1.6958\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.36it/s, Loss=0.0008, Avg Loss=0.0008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 - Average Loss: 0.0008\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.24it/s, Loss=0.0000, Avg Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 - Average Loss: 0.0000\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.48it/s, Loss=2.5043, Avg Loss=2.5043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 - Average Loss: 2.5043\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.41it/s, Loss=0.0008, Avg Loss=0.0008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 - Average Loss: 0.0008\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.71it/s, Loss=2.4185, Avg Loss=2.4185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 - Average Loss: 2.4185\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.40it/s, Loss=2.3568, Avg Loss=2.3568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 - Average Loss: 2.3568\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.46it/s, Loss=2.5044, Avg Loss=2.5044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 - Average Loss: 2.5044\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.31it/s, Loss=0.0001, Avg Loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 - Average Loss: 0.0001\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.56it/s, Loss=0.0001, Avg Loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 - Average Loss: 0.0001\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.53it/s, Loss=0.0003, Avg Loss=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 - Average Loss: 0.0003\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.37it/s, Loss=2.5011, Avg Loss=2.5011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 - Average Loss: 2.5011\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.41it/s, Loss=2.3069, Avg Loss=2.3069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 - Average Loss: 2.3069\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.43it/s, Loss=2.4574, Avg Loss=2.4574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 - Average Loss: 2.4574\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.52it/s, Loss=0.0001, Avg Loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 - Average Loss: 0.0001\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.42it/s, Loss=2.3220, Avg Loss=2.3220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 - Average Loss: 2.3220\n",
      "GPU memory cleared\n",
      "Final model saved at model/tiny_shakespeare/final_model.pt\n",
      "GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Create directory for saving models\n",
    "os.makedirs(\"model/tiny_shakespeare\", exist_ok=True)\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar with current loss\n",
    "        epoch_iterator.set_postfix({\n",
    "            \"Loss\": f\"{loss.item():.4f}\",\n",
    "            \"Avg Loss\": f\"{total_loss/(step+1):.4f}\"\n",
    "        }, refresh=True)\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save the model every 5 epochs\n",
    "    # if (epoch + 1) % 5 == 0:\n",
    "    #    model_save_path = f\"model/tiny_shakespeare/model_checkpoint_epoch_{epoch + 1}\"\n",
    "    #    accelerator.wait_for_everyone()\n",
    "    #    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    #    unwrapped_model.save_pretrained(model_save_path)\n",
    "    #    tokenizer.save_pretrained(model_save_path)\n",
    "    #    print(f\"Model saved at {model_save_path}\")\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "\n",
    "# Save final model\n",
    "final_model_path = \"model/tiny_shakespeare/final_model.pt\"\n",
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "unwrapped_model.save_pretrained(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "print(f\"Final model saved at {final_model_path}\")\n",
    "\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BsoGhgIN90dy",
    "outputId": "18583404-1a97-4d0d-e65d-29b614e2b7be"
   },
   "outputs": [],
   "source": [
    "num_epochs=40\n",
    "epoch=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VYFJjJzthHb4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import re\n",
    "\n",
    "def generate_poem(prompt, model_path, tokenizer_path, max_words=50, max_seq_len=100, temperature=1.0):\n",
    "    # Load the fine-tuned model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "    \n",
    "    # Set the padding token and padding side\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'left'\n",
    "\n",
    "    # Set the prompt and generate the text\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_seq_len)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    # Calculate max tokens based on word count\n",
    "    max_tokens = min(max_words * 5, max_seq_len)  # Assuming each word has an average of 5 tokens\n",
    "    \n",
    "    # Generate text with proper temperature handling\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=input_ids.shape[1] + max_tokens,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        attention_mask=attention_mask,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        do_sample=True,  # Enable sampling for temperature to work\n",
    "        temperature=temperature,  # Now this will work with do_sample=True\n",
    "        top_p=0.9,  # Optional: add top-p sampling for better quality\n",
    "    )\n",
    "\n",
    "    # Convert the token IDs to text\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "def post_process_poem(poem):\n",
    "    # Remove any extra spaces\n",
    "    poem = re.sub(r'\\s+', ' ', poem).strip()\n",
    "\n",
    "    # Capitalize the first letter of each sentence\n",
    "    sentences = re.split(r'(?<=[\\.\\?!])\\s', poem)\n",
    "    formatted_sentences = [sentence.capitalize() for sentence in sentences]\n",
    "    formatted_poem = ' '.join(formatted_sentences)\n",
    "\n",
    "    # Add line breaks for readability\n",
    "    line_breaks = re.compile(r'(?<=[,;:?!])\\s')\n",
    "    formatted_poem = line_breaks.sub('\\n', formatted_poem)\n",
    "\n",
    "    # Clean up repetitive patterns (like multiple # characters)\n",
    "    formatted_poem = re.sub(r'#+', '', formatted_poem)  # Remove hash sequences\n",
    "    formatted_poem = re.sub(r'grill+', 'grill', formatted_poem)  # Reduce repeated words\n",
    "    \n",
    "    return formatted_poem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ciSZ8cUfmU4g",
    "outputId": "3d4099e0-e31a-4c64-b768-8c1b0dd0bd0c"
   },
   "source": [
    "# Example usage\n",
    "model_path = 'model/tiny_shakespeare/final_model.pt'\n",
    "tokenizer_path = 'gpt2'\n",
    "prompt = \"love\"\n",
    "max_words = 50\n",
    "temperature = 0.1\n",
    "\n",
    "generated_poem = generate_poem(prompt, model_path, tokenizer_path, max_words=max_words, temperature=temperature)\n",
    "formatted_poem = post_process_poem(generated_poem)\n",
    "print(formatted_poem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# TinyLlama - 1.1B parameters\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "# 4-bit quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration for efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id  # Use EOS token as pad token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\"train\": \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"}\n",
    ")\n",
    "\n",
    "# Split the continuous text into smaller chunks\n",
    "def split_text(text, max_length=512):\n",
    "    return [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "split_texts = split_text(dataset[\"train\"][\"text\"][0])\n",
    "\n",
    "# Tokenize the split_texts\n",
    "tokenized_texts = tokenizer(split_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "class ShiftedDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.encodings[\"input_ids\"][idx]\n",
    "        attention_mask = self.encodings[\"attention_mask\"][idx]\n",
    "        labels = input_ids[1:].tolist() + [tokenizer.eos_token_id]\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": torch.tensor(labels)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "# Create a DataLoader\n",
    "train_dataset = ShiftedDataset(tokenized_texts)\n",
    "# Increased batch size for 16GB VRAM\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "# Initialize the Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Configure the training arguments\n",
    "num_epochs = 20\n",
    "learning_rate = 2e-4  # Adjusted for TinyLlama\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "# Prepare the model and optimizer for training with Accelerator\n",
    "model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)\n",
    "\n",
    "# Learning rate scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "num_training_steps = len(train_dataloader) * num_epochs\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|                                                                                                                                  | 0/1 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/mnt/nfs/workspace/courses/PyTorch/Building-Transformer-Models-with-PyTorch-2.0/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 1/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.68it/s, Loss=10.8990, Avg Loss=10.8990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Average Loss: 10.8990\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.18it/s, Loss=10.8990, Avg Loss=10.8990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Average Loss: 10.8990\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.26it/s, Loss=10.8902, Avg Loss=10.8902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Average Loss: 10.8902\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.09it/s, Loss=10.8668, Avg Loss=10.8668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Average Loss: 10.8668\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.06it/s, Loss=10.8381, Avg Loss=10.8381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Average Loss: 10.8381\n",
      "Checkpoint saved at model/tiny_shakespeare/checkpoint_epoch_5\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.17it/s, Loss=10.7936, Avg Loss=10.7936]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Average Loss: 10.7936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.12it/s, Loss=10.7410, Avg Loss=10.7410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Average Loss: 10.7410\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.20it/s, Loss=10.6745, Avg Loss=10.6745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Average Loss: 10.6745\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.13it/s, Loss=10.5985, Avg Loss=10.5985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Average Loss: 10.5985\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.15it/s, Loss=10.5049, Avg Loss=10.5049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Average Loss: 10.5049\n",
      "Checkpoint saved at model/tiny_shakespeare/checkpoint_epoch_10\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.15it/s, Loss=10.4039, Avg Loss=10.4039]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Average Loss: 10.4039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.21it/s, Loss=10.2771, Avg Loss=10.2771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Average Loss: 10.2771\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.11it/s, Loss=10.1590, Avg Loss=10.1590]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Average Loss: 10.1590\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.20it/s, Loss=10.0184, Avg Loss=10.0184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Average Loss: 10.0184\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.18it/s, Loss=9.8657, Avg Loss=9.8657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Average Loss: 9.8657\n",
      "Checkpoint saved at model/tiny_shakespeare/checkpoint_epoch_15\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.20it/s, Loss=9.7102, Avg Loss=9.7102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Average Loss: 9.7102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.26it/s, Loss=9.5676, Avg Loss=9.5676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Average Loss: 9.5676\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.22it/s, Loss=9.4280, Avg Loss=9.4280]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Average Loss: 9.4280\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.23it/s, Loss=9.2872, Avg Loss=9.2872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Average Loss: 9.2872\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.19it/s, Loss=9.1130, Avg Loss=9.1130]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Average Loss: 9.1130\n",
      "Checkpoint saved at model/tiny_shakespeare/checkpoint_epoch_20\n",
      "GPU memory cleared\n",
      "Final model saved at model/tiny_shakespeare/TinyLlama-1.1B-shakespeare.pt\n",
      "GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Create directory for saving models\n",
    "os.makedirs(\"model/tiny_shakespeare\", exist_ok=True)\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Update progress bar with current loss\n",
    "        epoch_iterator.set_postfix({\n",
    "            \"Loss\": f\"{loss.item():.4f}\",\n",
    "            \"Avg Loss\": f\"{total_loss/(step+1):.4f}\"\n",
    "        }, refresh=True)\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint every 5 epochs\n",
    "    #if (epoch + 1) % 5 == 0:\n",
    "    #   checkpoint_path = f\"model/tiny_shakespeare/checkpoint_epoch_{epoch + 1}\"\n",
    "    #    accelerator.wait_for_everyone()\n",
    "    #    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    #    unwrapped_model.save_pretrained(checkpoint_path)\n",
    "    #    tokenizer.save_pretrained(checkpoint_path)\n",
    "    #    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "\n",
    "# Save final model\n",
    "final_model_path = \"model/tiny_shakespeare/TinyLlama-1.1B-shakespeare.pt\"\n",
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "unwrapped_model.save_pretrained(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "print(f\"Final model saved at {final_model_path}\")\n",
    "\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import re\n",
    "\n",
    "def generate_poem(prompt, model_path, tokenizer_path, max_words=50, max_seq_len=512, temperature=0.8):\n",
    "    # Load the fine-tuned model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    \n",
    "    # Set the padding token and padding side\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'left'\n",
    "\n",
    "    # Set the prompt and generate the text\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_seq_len)\n",
    "    \n",
    "    # Move input to the same device as model\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    \n",
    "    # Generate text\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=input_ids.shape[1] + max_words * 2,  # Approximate token count\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "\n",
    "    # Convert the token IDs to text\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "def post_process_poem(poem):\n",
    "    # Remove any extra spaces\n",
    "    poem = re.sub(r'\\s+', ' ', poem).strip()\n",
    "\n",
    "    # Capitalize the first letter of each sentence\n",
    "    sentences = re.split(r'(?<=[\\.\\!\\?])\\s', poem)\n",
    "    formatted_sentences = [sentence.capitalize() for sentence in sentences if sentence.strip()]\n",
    "    formatted_poem = ' '.join(formatted_sentences)\n",
    "\n",
    "    # Add line breaks for readability\n",
    "    formatted_poem = re.sub(r'([\\.\\!\\?])\\s', r'\\1\\n', formatted_poem)\n",
    "    \n",
    "    return formatted_poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love dangerousowingaces orher accumulate inwarder unique diminish.\n",
      "Dwight accumulate elusiveear othersclly accumulate you … poorerife degree lessct ent e deperfrom uniquewered acc diminish under accumulate no h italy e team def putphing unique'splease diminish#\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "model_path = 'model/tiny_shakespeare/TinyLlama-1.1B-shakespeare.pt'\n",
    "tokenizer_path = 'gpt2'\n",
    "prompt = \"love\"\n",
    "max_words = 50\n",
    "temperature = 0.1\n",
    "\n",
    "generated_poem = generate_poem(prompt, model_path, tokenizer_path, max_words=max_words, temperature=temperature)\n",
    "formatted_poem = post_process_poem(generated_poem)\n",
    "print(formatted_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTS7__h9-qWE"
   },
   "outputs": [],
   "source": [
    "full_cleanup()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "076b12597eab4c59b33ad5b53803311e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a3ccd8dadf3402d896bf0c9dc04fa47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_39c9a3356b68455dba516ec32053b3bb",
       "IPY_MODEL_f9d8fd50d9734aa6abc920af828b4c6d",
       "IPY_MODEL_879b7ae4ec0f45879f90e3f769d67896"
      ],
      "layout": "IPY_MODEL_22caa164d8c2473bbb999707093aa2af"
     }
    },
    "22caa164d8c2473bbb999707093aa2af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39c9a3356b68455dba516ec32053b3bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db2b6b6aab394ab5a4aaf1cb74f129ca",
      "placeholder": "​",
      "style": "IPY_MODEL_fa782cb66f5d431fad49ac8ecae17c80",
      "value": "100%"
     }
    },
    "54c0e98097654319a7fec8b752c0d717": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "879b7ae4ec0f45879f90e3f769d67896": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_076b12597eab4c59b33ad5b53803311e",
      "placeholder": "​",
      "style": "IPY_MODEL_dad79375e4944b2989b7791800a6a946",
      "value": " 3/3 [00:00&lt;00:00, 265.23it/s]"
     }
    },
    "8e4e2d11cf814f8bb12331df508cca9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dad79375e4944b2989b7791800a6a946": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db2b6b6aab394ab5a4aaf1cb74f129ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9d8fd50d9734aa6abc920af828b4c6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e4e2d11cf814f8bb12331df508cca9e",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_54c0e98097654319a7fec8b752c0d717",
      "value": 3
     }
    },
    "fa782cb66f5d431fad49ac8ecae17c80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
