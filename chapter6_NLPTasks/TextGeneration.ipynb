{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.11 (main, Jul 23 2025, 00:34:44) [Clang 20.1.4 ]\n",
      "PyTorch: 2.8.0+cu129\n",
      "Transformers: 4.56.0\n",
      "Loaded utils from: /mnt/nfs/workspace/courses/PyTorch/Building-Transformer-Models-with-PyTorch-2.0/utils.py\n",
      "Memory environment configured\n",
      "Selected device: cuda\n",
      "GPU memory cleared\n",
      "Killed process 22687\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import platform\n",
    "\n",
    "# Set environment variable to help with memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Environment & version checks\n",
    "# Import CUDA utils from parent folder (preferred), fallback to local\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "try:\n",
    "    import torch, transformers\n",
    "    print(\"PyTorch:\", torch.__version__)\n",
    "    print(\"Transformers:\", transformers.__version__)\n",
    "except Exception as e:\n",
    "    print(\"You likely need to install torch/transformers:\", e)\n",
    "    \n",
    "# Try parent directory first (ideal location)\n",
    "parent_dir = str(Path.cwd().parent)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "try:\n",
    "    import utils  # expected at ../utils.py\n",
    "except Exception:\n",
    "    # Fallback: current working directory\n",
    "    curr_dir = str(Path.cwd())\n",
    "    if curr_dir not in sys.path:\n",
    "        sys.path.insert(0, curr_dir)\n",
    "    import utils  # tries ./utils.py\n",
    "\n",
    "from utils import *\n",
    "\n",
    "print(\"Loaded utils from:\", utils.__file__)\n",
    "# Set memory env & show current device\n",
    "utils.setup_memory_environment(expandable_segments=True)\n",
    "device = utils.get_device()\n",
    "print(\"Selected device:\", device)\n",
    "\n",
    "full_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "referenced_widgets": [
      "1a3ccd8dadf3402d896bf0c9dc04fa47",
      "39c9a3356b68455dba516ec32053b3bb",
      "f9d8fd50d9734aa6abc920af828b4c6d",
      "879b7ae4ec0f45879f90e3f769d67896",
      "22caa164d8c2473bbb999707093aa2af",
      "db2b6b6aab394ab5a4aaf1cb74f129ca",
      "fa782cb66f5d431fad49ac8ecae17c80",
      "8e4e2d11cf814f8bb12331df508cca9e",
      "54c0e98097654319a7fec8b752c0d717",
      "076b12597eab4c59b33ad5b53803311e",
      "dad79375e4944b2989b7791800a6a946"
     ]
    },
    "id": "Pv56f7UaYWAw",
    "outputId": "da0fc5c5-1f2e-428e-ed3e-326c8ab1ca21"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    " \n",
    "# TinyLlama - 1.1B parameters (small and fast)\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "# 4-bit quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id  # Use EOS token as pad token\n",
    "tokenizer.padding_side = \"left\"\n",
    " \n",
    "# Load the dataset\n",
    "#dataset = load_dataset(\"tiny_shakespeare\", revision=\"main\")\n",
    "# Load from the specific GitHub URL\n",
    "dataset = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\"train\": \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"}\n",
    ")\n",
    "\n",
    "'''\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['text'],\n",
    "        num_rows: 1\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['text'],\n",
    "        num_rows: 1\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['text'],\n",
    "        num_rows: 1\n",
    "    })\n",
    "})\n",
    "'''\n",
    "\n",
    "# Split the continuous text into smaller chunks\n",
    "def split_text(text, max_length=100):\n",
    "    return [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "# Apply the split_text function to the dataset\n",
    "\n",
    "\n",
    "split_texts = split_text(dataset[\"train\"][\"text\"][0])\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the split_texts\n",
    "tokenized_texts = tokenizer(split_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n",
    "\n",
    "class ShiftedDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.encodings[\"input_ids\"][idx]\n",
    "        attention_mask = self.encodings[\"attention_mask\"][idx]\n",
    "        labels = input_ids[1:].tolist() + [tokenizer.eos_token_id]\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": torch.tensor(labels)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "# Create a DataLoader\n",
    "train_dataset = ShiftedDataset(tokenized_texts)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kLjkG6s2YjZd",
    "outputId": "8a607c9c-c8a9-4b3b-9b6e-a902365fc499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  3824, 21353, 19642, 29901]])\n",
      "tensor([[1, 1, 1, 1, 1]])\n",
      "tensor([[ 3824, 21353, 19642, 29901,     2]])\n"
     ]
    }
   ],
   "source": [
    "item=next(iter(train_dataloader))\n",
    "print(item['input_ids'])\n",
    "print(item['attention_mask'])\n",
    "print(item['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FWMR85OQYtdS"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Initialize the Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Configure the training arguments\n",
    "num_epochs = 20\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Initialize the GPT-2 model and optimizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Prepare the model and optimizer for training with Accelerator\n",
    "model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BvY6jfFIlv_E"
   },
   "outputs": [],
   "source": [
    "num_epochs=40\n",
    "epoch=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fKdAzfSRgMKu",
    "outputId": "63ce3a8d-d0db-4be4-e456-d72b86a09b25"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40:   0%|                                                                                                                                                                | 0/1 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "Epoch 1/40: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.77it/s, Loss=14.4406, Avg Loss=14.4406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 - Average Loss: 14.4406\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.90it/s, Loss=12.9112, Avg Loss=12.9112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 - Average Loss: 12.9112\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.29it/s, Loss=10.0546, Avg Loss=10.0546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 - Average Loss: 10.0546\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.50it/s, Loss=9.0267, Avg Loss=9.0267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 - Average Loss: 9.0267\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.43it/s, Loss=6.4970, Avg Loss=6.4970]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 - Average Loss: 6.4970\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.40it/s, Loss=7.4582, Avg Loss=7.4582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 - Average Loss: 7.4582\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.48it/s, Loss=6.9754, Avg Loss=6.9754]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 - Average Loss: 6.9754\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.25it/s, Loss=6.2760, Avg Loss=6.2760]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 - Average Loss: 6.2760\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.54it/s, Loss=2.0550, Avg Loss=2.0550]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 - Average Loss: 2.0550\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.28it/s, Loss=2.2235, Avg Loss=2.2235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 - Average Loss: 2.2235\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.55it/s, Loss=0.4841, Avg Loss=0.4841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 - Average Loss: 0.4841\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.43it/s, Loss=0.1839, Avg Loss=0.1839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 - Average Loss: 0.1839\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.41it/s, Loss=0.1890, Avg Loss=0.1890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 - Average Loss: 0.1890\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.35it/s, Loss=0.0141, Avg Loss=0.0141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 - Average Loss: 0.0141\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.38it/s, Loss=0.0247, Avg Loss=0.0247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 - Average Loss: 0.0247\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.41it/s, Loss=0.0098, Avg Loss=0.0098]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 - Average Loss: 0.0098\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.37it/s, Loss=0.0174, Avg Loss=0.0174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 - Average Loss: 0.0174\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.38it/s, Loss=0.0020, Avg Loss=0.0020]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 - Average Loss: 0.0020\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.61it/s, Loss=0.0013, Avg Loss=0.0013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 - Average Loss: 0.0013\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.45it/s, Loss=2.6749, Avg Loss=2.6749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 - Average Loss: 2.6749\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.51it/s, Loss=0.0212, Avg Loss=0.0212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 - Average Loss: 0.0212\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.45it/s, Loss=0.0014, Avg Loss=0.0014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 - Average Loss: 0.0014\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.45it/s, Loss=0.0018, Avg Loss=0.0018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 - Average Loss: 0.0018\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.26it/s, Loss=0.0007, Avg Loss=0.0007]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 - Average Loss: 0.0007\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.45it/s, Loss=1.6958, Avg Loss=1.6958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 - Average Loss: 1.6958\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.36it/s, Loss=0.0008, Avg Loss=0.0008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 - Average Loss: 0.0008\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.24it/s, Loss=0.0000, Avg Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 - Average Loss: 0.0000\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.48it/s, Loss=2.5043, Avg Loss=2.5043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 - Average Loss: 2.5043\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.41it/s, Loss=0.0008, Avg Loss=0.0008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 - Average Loss: 0.0008\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.71it/s, Loss=2.4185, Avg Loss=2.4185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 - Average Loss: 2.4185\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.40it/s, Loss=2.3568, Avg Loss=2.3568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 - Average Loss: 2.3568\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.46it/s, Loss=2.5044, Avg Loss=2.5044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 - Average Loss: 2.5044\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.31it/s, Loss=0.0001, Avg Loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 - Average Loss: 0.0001\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.56it/s, Loss=0.0001, Avg Loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 - Average Loss: 0.0001\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.53it/s, Loss=0.0003, Avg Loss=0.0003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 - Average Loss: 0.0003\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.37it/s, Loss=2.5011, Avg Loss=2.5011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 - Average Loss: 2.5011\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.41it/s, Loss=2.3069, Avg Loss=2.3069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 - Average Loss: 2.3069\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.43it/s, Loss=2.4574, Avg Loss=2.4574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 - Average Loss: 2.4574\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.52it/s, Loss=0.0001, Avg Loss=0.0001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 - Average Loss: 0.0001\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.42it/s, Loss=2.3220, Avg Loss=2.3220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 - Average Loss: 2.3220\n",
      "GPU memory cleared\n",
      "Final model saved at model/tiny_shakespeare/final_model.pt\n",
      "GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Create directory for saving models\n",
    "os.makedirs(\"model/tiny_shakespeare\", exist_ok=True)\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    epoch_iterator = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar with current loss\n",
    "        epoch_iterator.set_postfix({\n",
    "            \"Loss\": f\"{loss.item():.4f}\",\n",
    "            \"Avg Loss\": f\"{total_loss/(step+1):.4f}\"\n",
    "        }, refresh=True)\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save the model every 5 epochs\n",
    "    # if (epoch + 1) % 5 == 0:\n",
    "    #    model_save_path = f\"model/tiny_shakespeare/model_checkpoint_epoch_{epoch + 1}\"\n",
    "    #    accelerator.wait_for_everyone()\n",
    "    #    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    #    unwrapped_model.save_pretrained(model_save_path)\n",
    "    #    tokenizer.save_pretrained(model_save_path)\n",
    "    #    print(f\"Model saved at {model_save_path}\")\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "\n",
    "# Save final model\n",
    "final_model_path = \"model/tiny_shakespeare/final_model.pt\"\n",
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "unwrapped_model.save_pretrained(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "print(f\"Final model saved at {final_model_path}\")\n",
    "\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BsoGhgIN90dy",
    "outputId": "18583404-1a97-4d0d-e65d-29b614e2b7be"
   },
   "outputs": [],
   "source": [
    "num_epochs=40\n",
    "epoch=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VYFJjJzthHb4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import re\n",
    "\n",
    "def generate_poem(prompt, model_path, tokenizer_path, max_words=50, max_seq_len=100, temperature=1.0):\n",
    "    # Load the fine-tuned model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "    \n",
    "    # Set the padding token and padding side\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'left'\n",
    "\n",
    "    # Set the prompt and generate the text\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_seq_len)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    # Calculate max tokens based on word count\n",
    "    max_tokens = min(max_words * 5, max_seq_len)  # Assuming each word has an average of 5 tokens\n",
    "    \n",
    "    # Generate text with proper temperature handling\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=input_ids.shape[1] + max_tokens,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        attention_mask=attention_mask,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        do_sample=True,  # Enable sampling for temperature to work\n",
    "        temperature=temperature,  # Now this will work with do_sample=True\n",
    "        top_p=0.9,  # Optional: add top-p sampling for better quality\n",
    "    )\n",
    "\n",
    "    # Convert the token IDs to text\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "def post_process_poem(poem):\n",
    "    # Remove any extra spaces\n",
    "    poem = re.sub(r'\\s+', ' ', poem).strip()\n",
    "\n",
    "    # Capitalize the first letter of each sentence\n",
    "    sentences = re.split(r'(?<=[\\.\\?!])\\s', poem)\n",
    "    formatted_sentences = [sentence.capitalize() for sentence in sentences]\n",
    "    formatted_poem = ' '.join(formatted_sentences)\n",
    "\n",
    "    # Add line breaks for readability\n",
    "    line_breaks = re.compile(r'(?<=[,;:?!])\\s')\n",
    "    formatted_poem = line_breaks.sub('\\n', formatted_poem)\n",
    "\n",
    "    # Clean up repetitive patterns (like multiple # characters)\n",
    "    formatted_poem = re.sub(r'#+', '', formatted_poem)  # Remove hash sequences\n",
    "    formatted_poem = re.sub(r'grill+', 'grill', formatted_poem)  # Reduce repeated words\n",
    "    \n",
    "    return formatted_poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ciSZ8cUfmU4g",
    "outputId": "3d4099e0-e31a-4c64-b768-8c1b0dd0bd0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lovenature grill grill grill  grill_  bbq grill grilling grill grilled grill grill grill grilling grill grilled grillgrilling  grill grilled grills grillgrilled grill grill bbq grill grillsgrills grilla grill barbecue grill grilling grille grillgrilling grillegrills grilled grilled griller grill  grillbr grill br grill br grillbr grill br br  grillb grill b grill grill  grill\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "model_path = 'model/tiny_shakespeare/final_model.pt'\n",
    "tokenizer_path = 'gpt2'\n",
    "prompt = \"love\"\n",
    "max_words = 50\n",
    "temperature = 0.1\n",
    "\n",
    "generated_poem = generate_poem(prompt, model_path, tokenizer_path, max_words=max_words, temperature=temperature)\n",
    "formatted_poem = post_process_poem(generated_poem)\n",
    "print(formatted_poem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTS7__h9-qWE"
   },
   "outputs": [],
   "source": [
    "full_cleanup()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "076b12597eab4c59b33ad5b53803311e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a3ccd8dadf3402d896bf0c9dc04fa47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_39c9a3356b68455dba516ec32053b3bb",
       "IPY_MODEL_f9d8fd50d9734aa6abc920af828b4c6d",
       "IPY_MODEL_879b7ae4ec0f45879f90e3f769d67896"
      ],
      "layout": "IPY_MODEL_22caa164d8c2473bbb999707093aa2af"
     }
    },
    "22caa164d8c2473bbb999707093aa2af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39c9a3356b68455dba516ec32053b3bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db2b6b6aab394ab5a4aaf1cb74f129ca",
      "placeholder": "​",
      "style": "IPY_MODEL_fa782cb66f5d431fad49ac8ecae17c80",
      "value": "100%"
     }
    },
    "54c0e98097654319a7fec8b752c0d717": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "879b7ae4ec0f45879f90e3f769d67896": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_076b12597eab4c59b33ad5b53803311e",
      "placeholder": "​",
      "style": "IPY_MODEL_dad79375e4944b2989b7791800a6a946",
      "value": " 3/3 [00:00&lt;00:00, 265.23it/s]"
     }
    },
    "8e4e2d11cf814f8bb12331df508cca9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dad79375e4944b2989b7791800a6a946": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db2b6b6aab394ab5a4aaf1cb74f129ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9d8fd50d9734aa6abc920af828b4c6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e4e2d11cf814f8bb12331df508cca9e",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_54c0e98097654319a7fec8b752c0d717",
      "value": 3
     }
    },
    "fa782cb66f5d431fad49ac8ecae17c80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
