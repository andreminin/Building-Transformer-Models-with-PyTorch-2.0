{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "015e998f",
   "metadata": {},
   "source": [
    "\n",
    "# Refactored: Transfer Learning — Qwen 2.5 + Hugging Face Transformers + CUDA utils\n",
    "\n",
    "This top section upgrades the notebook to use **Qwen 2.5** models via `transformers` and integrates CUDA management utilities from `utils.py` (preferred at parent folder).  \n",
    "It does **not** delete your original notebook content — it adds a modernized, ready-to-run section for transfer learning and model loading. Change `model_id` as needed for model size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa6f4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.11 (main, Jul 23 2025, 00:34:44) [Clang 20.1.4 ]\n",
      "PyTorch: 2.8.0+cu129\n",
      "Transformers: 4.56.0\n",
      "Loaded utils from: /mnt/nfs/workspace/courses/PyTorch/Building-Transformer-Models-with-PyTorch-2.0/utils.py\n",
      "Memory environment configured\n",
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import platform\n",
    "\n",
    "# Set environment variable to help with memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Environment & version checks\n",
    "# Import CUDA utils from parent folder (preferred), fallback to local\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "try:\n",
    "    import torch, transformers\n",
    "    print(\"PyTorch:\", torch.__version__)\n",
    "    print(\"Transformers:\", transformers.__version__)\n",
    "except Exception as e:\n",
    "    print(\"You likely need to install torch/transformers:\", e)\n",
    "    \n",
    "# Try parent directory first (ideal location)\n",
    "parent_dir = str(Path.cwd().parent)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "try:\n",
    "    import utils  # expected at ../utils.py\n",
    "except Exception:\n",
    "    # Fallback: current working directory\n",
    "    curr_dir = str(Path.cwd())\n",
    "    if curr_dir not in sys.path:\n",
    "        sys.path.insert(0, curr_dir)\n",
    "    import utils  # tries ./utils.py\n",
    "\n",
    "from utils import *\n",
    "\n",
    "print(\"Loaded utils from:\", utils.__file__)\n",
    "# Set memory env & show current device\n",
    "utils.setup_memory_environment(expandable_segments=True)\n",
    "device = utils.get_device()\n",
    "print(\"Selected device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f8bad11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared\n",
      "Loading tokenizer and model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "Model loaded. Device: cuda:0\n",
      "=== GPU Memory Usage ===\n",
      "Allocated: 0.92 GB\n",
      "Reserved:  1.86 GB\n",
      "Free:      14.56 GB\n",
      "Total:     15.48 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -- Model selection and load (Qwen 2.5) --\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"  # change to desired Qwen 2.5 variant\n",
    "gen_kwargs = {\"max_new_tokens\": 256, \"temperature\": 0.7, \"top_p\": 0.9, \"do_sample\": True}\n",
    "\n",
    "# clear GPU before load\n",
    "if hasattr(utils, \"clear_gpu_memory\"):\n",
    "    try:\n",
    "        utils.clear_gpu_memory(aggressive=False)\n",
    "    except Exception as e:\n",
    "        print(\"clear_gpu_memory failed:\", e)\n",
    "\n",
    "use_bfloat16 = False\n",
    "try:\n",
    "    if device == \"cuda\" and torch.cuda.is_available():\n",
    "        use_bfloat16 = torch.cuda.is_bf16_supported()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "dtype = torch.bfloat16 if use_bfloat16 else (torch.float16 if device == \"cuda\" else None)\n",
    "\n",
    "print(\"Loading tokenizer and model:\", model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    dtype=(dtype if dtype is not None else None),  # Changed from torch_dtype to dtype\n",
    "    device_map=(\"auto\" if device in (\"cuda\", \"mps\") else None)\n",
    ")\n",
    "model.eval()\n",
    "print(\"Model loaded. Device:\", model.device)\n",
    "if hasattr(utils, \"print_cuda_memory\"):\n",
    "    try:\n",
    "        utils.print_cuda_memory(verbose=True)\n",
    "    except Exception as e:\n",
    "        print(\"print_cuda_memory failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c72565",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -- Simple transfer learning / fine-tuning scaffold note --\n",
    "# This notebook is named Transfer_Learning.ipynb, so below are recommended starting points.\n",
    "# Use the existing dataset handling cells in the original notebook — these refactor cells only\n",
    "# ensure modern model loading and CUDA helpers are available.\n",
    "#\n",
    "# Typical steps:\n",
    "# 1. Prepare dataset -> map to chat prompts using tokenizer.apply_chat_template (if instruct-style)\n",
    "# 2. Create tokenized dataset with labels (language modelling)\n",
    "# 3. Use DataCollatorForLanguageModeling or custom collator\n",
    "# 4. Use Trainer or accelerate + custom training loop; consider PEFT/LoRA for efficiency\n",
    "#\n",
    "# See the earlier created chatbot_MOD.ipynb for a full example of training args and LOra scaffolding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e442e",
   "metadata": {
    "id": "y-o9q5TSzR8a"
   },
   "source": [
    "We will build the real news vs fake news detection engine. We want to demonstrate how this pipeline can be adapted to your organization's specific needs. Instead of using a pre-built dataset, we will download a dataset from Kaggle and utilize it in our fine-tuning process. This approach will help illustrate how the pipeline can be tailored to work with custom datasets in real-world applications.\n",
    "Here's an outline of the fine-tuning process\n",
    "1. Import required libraries and packages\n",
    "\n",
    "2. Load the dataset. Download the data from kaggle and save it on your drive.\n",
    "3. Load pre-trained BERT tokenizer:\n",
    "\n",
    "\n",
    "4. Prepare the dataset: \n",
    "\n",
    "\n",
    "  * Tokenize the text using the BERT tokenizer\n",
    "  * Create attention masks\n",
    " * Split the dataset into training and validation sets\n",
    "  * Create a custom PyTorch dataset class (TextClassificationDataset)\n",
    "  * Instantiate the custom dataset for both training and validation sets\n",
    "  * Create PyTorch DataLoader\n",
    "  \n",
    "4. Load a pre-trained BERT model for sequence classification using the Hugging Face Transformers library\n",
    "5. Setup Accelarator environment\n",
    "6. Fine-tune the model:\n",
    "\n",
    "7. Evaluate the model:\n",
    "  *Calculate  metrics, such as F1 score, recall, and precision\n",
    "8. Inference:\n",
    "\n",
    "  * Create a function to perform inference on new text input\n",
    " * Tokenize the input text and convert it to the required format\n",
    " * Perform inference using the fine-tuned model\n",
    " * Interpret the model's output and return the predicted class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5bc2ee",
   "metadata": {
    "id": "WrC8FHao2aon"
   },
   "source": [
    "# 1. Import required libraries and packages\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "379c7922",
   "metadata": {
    "id": "rOit_UrJdw9-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Now import and run your code\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "# Your model loading and training code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b33ec09",
   "metadata": {
    "id": "h2Z31EI_y08Q"
   },
   "source": [
    "**Note:** MPS=> Apple's Metal Performance Shaders (MPS) is a framework that provides highly optimized, low-level GPU-accelerated functions for deep learning, image processing, and other compute-intensive tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fdbd5cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XdAE-Xc4cg9D",
    "outputId": "d7ef1852-9464-4076-854c-2d5b45d2a94b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "  device=\"cpu\"\n",
    "  if torch.cuda.is_available():\n",
    "    device=\"cuda\"\n",
    "  elif  torch.backends.mps.is_available():\n",
    "    device='mps'\n",
    "  else:\n",
    "    device=\"cpu\"\n",
    "  return device\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "print(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcbc5f6",
   "metadata": {
    "id": "aOT9-e3R15hG"
   },
   "source": [
    "# 2. Load Data\n",
    "1. Reading data from two CSV files: True.csv (real news) and Fake.csv (fake news)\n",
    "2. Cleaning and preprocessing the data in each CSV file\n",
    "3. Concatenating both dataframes into a single dataframe\n",
    "4. The resulting dataframe contains two columns: 'text' for the news content and 'label' for its corresponding category (real or fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90ad8c0c",
   "metadata": {
    "id": "f2YociOdJ4CD"
   },
   "outputs": [],
   "source": [
    "real=pd.read_csv('true.csv')\n",
    "fake=pd.read_csv('fake.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00b56b1d",
   "metadata": {
    "id": "IERhOScoLJM9"
   },
   "outputs": [],
   "source": [
    "real = real.drop(['title','subject','date'], axis=1)\n",
    "real['label']=1.0\n",
    "fake = fake.drop(['title','subject','date'], axis=1)\n",
    "fake['label']=0.0\n",
    "dataframe=pd.concat([real, fake], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e2b8500",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yBU1b0_syl00",
    "outputId": "df0fbc86-91e6-4d46-af49-312160b52436",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  label\n",
      "0   (Reuters) - President Donald Trump has cemente...    1.0\n",
      "1   Republicans have revived an old and overused p...    0.0\n",
      "2   Remember when experts came out after Hillary c...    0.0\n",
      "3   If anyone wonders how the  Reverend  Al Sharpt...    0.0\n",
      "4   Just two months shy of the one-year anniversar...    0.0\n",
      "5   JOHANNESBURG (Reuters) - South Africa s Nation...    1.0\n",
      "6   You can often figure a lot out about a person ...    0.0\n",
      "7   WASHINGTON (Reuters) - U.S. House Democratic L...    1.0\n",
      "8   WASHINGTON (Reuters) - U.S. House Armed Servic...    1.0\n",
      "9   Besty Devos is Trump s conservative choice for...    0.0\n",
      "10  WASHINGTON (Reuters) - President Donald Trump’...    1.0\n",
      "11  Our country is spinning out of control. Obama ...    0.0\n",
      "12  CLEVELAND (Reuters) - U.S. House Speaker Paul ...    1.0\n",
      "13  CHICAGO (Reuters) - Illinois’ long-running bud...    1.0\n",
      "14  It won t be long before the progressives start...    0.0\n",
      "15  Donald Trump can brag all he wants, but Rachel...    0.0\n",
      "16  BOGOTA (Reuters) - Colombia and the Marxist EL...    1.0\n",
      "17  The Feds are looking at several states to tran...    0.0\n",
      "18  LA PAZ (Reuters) - Thousands of Bolivians marc...    1.0\n",
      "19   This is not Charlotte that s out here. These ...    0.0\n",
      "2167\n",
      "2323\n"
     ]
    }
   ],
   "source": [
    "df = dataframe.sample(frac=0.1).reset_index(drop=True)\n",
    "print(df.head(20))\n",
    "print(len(df[df['label']==1.0]))\n",
    "print(len(df[df['label']==0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec05838",
   "metadata": {
    "id": "UEE-QEUW3YyM"
   },
   "source": [
    "#3.  Load Tokenizer:\n",
    "1. We are using the `bert-base-uncased` tokenizer. We also need to use the corresponding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dbc942f",
   "metadata": {
    "id": "AFP8kHXrzfSL"
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27878b0f",
   "metadata": {
    "id": "4SQ3sVVn3tSi"
   },
   "source": [
    "# 4. Prepare Data\n",
    "The data preparation process for BERT-based uncased models involves tokenizing the text, mapping tokens to `input_ids`, creating attention masks `attention_mask`, , and preparing the labels tensor `labels`. Each element of Dataset Class should be dictionary of following structure.\n",
    "\n",
    "```\n",
    "{'input_ids': torch.Tensor(),'attention_mask':torch.Tensor(), 'labels': torch.Tensor()  }\n",
    "```\n",
    "1. Tokenization: The text input should be tokenized into subwords using BERT's WordPiece tokenizer. This tokenizer converts the text into a format that BERT can understand.\n",
    "\n",
    "2. `input_ids`: Each token from the tokenized text needs to be mapped to an ID using BERT's vocabulary. The resulting input IDs should be in the form of a tensor or array, usually of shape (batch_size, max_sequence_length).\n",
    "3. `attention_mask`: The attention mask is used to differentiate between the actual tokens and padding tokens. It has the same shape as the input IDs tensor, i.e., (batch_size, max_sequence_length). The mask has 1s for actual tokens and 0s for padding tokens.\n",
    "4. `labels`: The labels tensor contains the true class or value for each example in the dataset. It usually has a shape of (batch_size,). For classification tasks, these labels are one-hot-encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9472cbd0",
   "metadata": {
    "id": "jbJiQDfw9tUI"
   },
   "outputs": [],
   "source": [
    "# this is just creating list of tuples. Each tupe has (text, label)\n",
    "data=list(zip(df['text'].tolist(), df['label'].tolist()))\n",
    "\n",
    "# This function takes list of Texts, and Labels as Parameter\n",
    "# This function return input_ids, attention_mask, and labels_out\n",
    "def tokenize_and_encode(texts, labels):\n",
    "    input_ids, attention_masks, labels_out = [], [], []\n",
    "    for text, label in zip(texts, labels):\n",
    "        encoded = tokenizer.encode_plus(text, max_length=512, padding='max_length', truncation=True)\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "        labels_out.append(label)\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_masks), torch.tensor(labels_out)\n",
    "\n",
    "# seprate the tuples\n",
    "# generate two lists: a) containing texts, b) containing labels\n",
    "texts, labels = zip(*data)\n",
    "\n",
    "# train, validation split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# tokenization\n",
    "train_input_ids, train_attention_masks, train_labels = tokenize_and_encode(train_texts, train_labels)\n",
    "val_input_ids, val_attention_masks, val_labels = tokenize_and_encode(val_texts, val_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea164127",
   "metadata": {
    "id": "A0sp2xdP7UPG"
   },
   "source": [
    "**It's always good to review the data**\n",
    "1. input_ids\n",
    "  * `0` token value means padded token\n",
    "2. attention_mask\n",
    "  * `1`: corresponding token is real token\n",
    "  * `0`: corresponding token is padded token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2034fa39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9VcMmQEeQ9H",
    "outputId": "d943ed4e-5694-4345-9ff3-cbde0b561bef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input_ids  torch.Size([512]) tensor([101, 102,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0]) \n",
      "train_attention_masks  tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]) tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]) \n",
      "train_labels tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print('train_input_ids ',train_input_ids[0].shape ,train_input_ids[0], '\\n'\n",
    "      'train_attention_masks ', train_attention_masks[0] ,train_attention_masks[0], '\\n'\n",
    "      'train_labels', train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ef6407",
   "metadata": {
    "id": "ZTyeVXy7t-T1"
   },
   "source": [
    "### TextClassificationDataset\n",
    "1. For tunning `bert-based-uncased`: each item of Dataset must be of type dictionary with at following  keys:\n",
    "  * input_ids\n",
    "  * attention_mask\n",
    "  * labels\n",
    "2. Thus,  `__getitem__`  should return dictionary of following structure:\n",
    "```\n",
    "{\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.one_hot_labels[idx]\n",
    "        }\n",
    "```\n",
    "3. one_hot_encode method: A static method that takes in targets (labels) and num_classes as arguments. It converts the given targets into one-hot encoded tensors. The method first converts the targets to long tensors and then initializes a zero tensor of shape (number of samples, num_classes). The scatter_ function is used to place 1.0 in the appropriate position for each sample's label, resulting in a one-hot encoded tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f18cf78",
   "metadata": {
    "id": "7yfXP_F2zVcs"
   },
   "outputs": [],
   "source": [
    "class TextClassificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels, num_classes=2):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "        self.num_classes = num_classes\n",
    "        self.one_hot_labels = self.one_hot_encode(labels, num_classes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.one_hot_labels[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot_encode(targets, num_classes):\n",
    "        targets = targets.long()\n",
    "        one_hot_targets = torch.zeros(targets.size(0), num_classes)\n",
    "        one_hot_targets.scatter_(1, targets.unsqueeze(1), 1.0)\n",
    "        return one_hot_targets\n",
    "        \n",
    "\n",
    "train_dataset = TextClassificationDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "val_dataset = TextClassificationDataset(val_input_ids, val_attention_masks, val_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8777b9",
   "metadata": {
    "id": "KQxJ0TU8vGT1"
   },
   "source": [
    "### DataLoader\n",
    "*italicized text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "891da1e2",
   "metadata": {
    "id": "ZmUysho6O8kp"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce943d63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6GOjnpbz0Na",
    "outputId": "594909ce-782f-434a-ffba-d75ca5fc8e84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "898"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "len((val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b65f28f",
   "metadata": {
    "id": "7i-QtecpxXAs"
   },
   "source": [
    "1.Revisiting dimension requirements for Transformers in Pytorch from Chapter 3: The encoder expects data with dimensions (seq_len, batch_size). However, Hugging Face's bert-based-uncased model requires data with dimensions (batch_size, seq_len). As a result, the output from the train_dataloader has dimensions of (batch_size, seq_len)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43c9124e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QhpJsZniecpV",
    "outputId": "205e6918-cd87-4637-d098-21ddb13814ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_ids,  torch.Size([2, 512]) \n",
      " item_mask,  torch.Size([2, 512]) \n",
      " item_labels,  torch.Size([2, 2]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "item=next(iter(train_dataloader))\n",
    "item_ids,item_mask,item_labels=item['input_ids'],item['attention_mask'],item['labels']\n",
    "print ('item_ids, ',item_ids.shape, '\\n',\n",
    "       'item_mask, ',item_mask.shape, '\\n',\n",
    "       'item_labels, ',item_labels.shape, '\\n',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b77b91e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de0AgV2QaYy4",
    "outputId": "f3360699-0a0b-4319-9e17-9872b318a52b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66d012d",
   "metadata": {
    "id": "lPDCD6Em3R5n"
   },
   "source": [
    "# 5. Prepare Accelaerator\n",
    "What is Accelerator?\n",
    " 1. It provides an easy-to-use API for training deep learning models on various hardware accelerators, such as GPUs, TPUs, and Apple's Metal Performance Shaders (MPS).\n",
    "  * In our example, during training, we donot specifically select 'mps' device. THe accelerator automatically detects it and use 'mps' for training\n",
    " 2. The Accelerator library is particularly useful for distributed training and mixed-precision training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28c2a30c",
   "metadata": {
    "id": "GK9hKmD6OOoK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from utils import *\n",
    "\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Now try to prepare the model\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db064053",
   "metadata": {
    "id": "e_3lyEAH34rv"
   },
   "source": [
    "# 5. Fine Tune The Model\n",
    "1. `lr_scheduler` in the provided code is an instance of a learning rate scheduler, which is responsible for adjusting the learning rate during the training process. The learning rate scheduler helps improve the training process by dynamically adjusting the learning rate based on the number of training steps. In this code, the learning rate starts with the initial value set in the optimizer and decreases linearly to 0 as the training progresses.\n",
    "2. Some benefit of lr_scheduler over optimizer alone are\n",
    "  * Faster convergence \n",
    "  * Avoid Overshooting: When using a fixed learning rate, the optimizer might overshoot the optimal solution, especially in the later stages of training. By decreasing the learning rate over time, the model can make smaller updates and fine-tune its weights\n",
    "  \n",
    "3. `progress_bar` is just utility to show the progress of training\n",
    "4. These are standard approach for fine tunning:\n",
    "```\n",
    " }\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "```\n",
    "  * each batch should be dictionary of structure {input_ids:torch.Tensor(), attention_mask: torch.Tensor(), labels: torch.Tensor()\n",
    "  * the dimension of input_ids=(batch_size, seq_len); attention_mask= (batch_size, seq_len); and labels=(batch_size,)\n",
    "  * You can notice that during training, we are not explicitly converting `tensor` into device; accelerator is automatically identifying the `device` and converting `tensor` into the appropriate format\n",
    "1. After each epoch, we are also printing the evaluation metrics over the evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "811f6623",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UaDCyPtoQlZR",
    "outputId": "8dc754e0-1cfc-48f2-827f-c899313dbbda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory environment configured\n",
      "GPU memory cleared\n",
      "Using device: cuda\n",
      "=== GPU Memory Usage ===\n",
      "Allocated: 0.41 GB\n",
      "Reserved:  0.41 GB\n",
      "Free:      15.07 GB\n",
      "Total:     15.48 GB\n",
      "GPU memory cleared\n",
      "Available memory: 11.97GB\n",
      "Using batch size: 29\n",
      "Using batch size: 29\n",
      "Using gradient accumulation steps: 1\n",
      "\n",
      "=== Epoch 1/3 ===\n",
      "GPU Memory: 0.41GB used, 15.07GB free, 15.48GB total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training: 100%|█| 124/124 [01:43<00:00\n",
      "Epoch 1/3 - Evaluating: 100%|█| 31/31 [00:08<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3 Summary:\n",
      "Train Loss: 0.3644 | Accuracy: 0.9933 | F1: 0.9933 | Recall: 0.9933 | Precision: 0.9934\n",
      "GPU memory cleared\n",
      "\n",
      "=== Epoch 2/3 ===\n",
      "GPU Memory: 1.24GB used, 14.24GB free, 15.48GB total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Training: 100%|█| 124/124 [01:43<00:00\n",
      "Epoch 2/3 - Evaluating: 100%|█| 31/31 [00:08<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/3 Summary:\n",
      "Train Loss: 0.0249 | Accuracy: 0.9978 | F1: 0.9978 | Recall: 0.9978 | Precision: 0.9978\n",
      "GPU memory cleared\n",
      "\n",
      "=== Epoch 3/3 ===\n",
      "GPU Memory: 1.24GB used, 14.24GB free, 15.48GB total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Training: 100%|█| 124/124 [01:43<00:00\n",
      "Epoch 3/3 - Evaluating: 100%|█| 31/31 [00:08<00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/3 Summary:\n",
      "Train Loss: 0.0102 | Accuracy: 0.9978 | F1: 0.9978 | Recall: 0.9978 | Precision: 0.9978\n",
      "GPU memory cleared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from tqdm import tqdm\n",
    "# Direct import from parent folder\n",
    "import importlib.util\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# import utils directly\n",
    "from utils import *\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Setup memory environment\n",
    "setup_memory_environment()\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Get device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Print memory info\n",
    "print_cuda_memory(verbose=True)\n",
    "\n",
    "# Calculate optimal batch size\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "optimal_batch_size = calculate_optimal_batch_size_simple(model, sample_batch)\n",
    "#optimal_batch_size = get_optimal_batch_size()\n",
    "\n",
    "# Recreate dataloaders with optimal batch size\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=optimal_batch_size, shuffle=True)\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=optimal_batch_size)\n",
    "\n",
    "print(f\"Using batch size: {optimal_batch_size}\")\n",
    "\n",
    "# Set accumulation steps\n",
    "target_effective_batch_size = 8\n",
    "accumulation_steps = max(1, target_effective_batch_size // optimal_batch_size)\n",
    "print(f\"Using gradient accumulation steps: {accumulation_steps}\")\n",
    "\n",
    "num_epochs = 3\n",
    "accumulation_steps = 4  # Effective batch size = 2 * 4 = 8\n",
    "num_training_steps = num_epochs * len(train_dataloader) // accumulation_steps\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Let accelerator prepare everything\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "# Training loop with memory monitoring\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n=== Epoch {epoch+1}/{num_epochs} ===\")\n",
    "    print_cuda_memory()\n",
    "    \n",
    "    # Training phase...\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    bar_length = get_optimal_bar_length()\n",
    "    \n",
    "    train_progress_bar = tqdm(\n",
    "        train_dataloader,\n",
    "        desc=f\"Epoch {epoch+1}/{num_epochs} - Training\",\n",
    "        unit=\"batch\",\n",
    "        ncols=bar_length,\n",
    "        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'\n",
    "    )\n",
    "    \n",
    "   # train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} \")\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(train_progress_bar):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss / accumulation_steps  # Scale loss\n",
    "        accelerator.backward(loss)\n",
    "        total_train_loss += loss.item() * accumulation_steps\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update progress bar with current loss\n",
    "            avg_loss = total_train_loss / (i + 1)\n",
    "            train_progress_bar.set_postfix({\n",
    "                'loss': f'{avg_loss:.4f}',\n",
    "                'lr': f'{lr_scheduler.get_last_lr()[0]:.2e}'\n",
    "            })\n",
    "    \n",
    "    # Evaluation phase\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    out_label_ids = []\n",
    "\n",
    "    bar_length = get_optimal_bar_length()\n",
    "    \n",
    "    eval_progress_bar = tqdm(\n",
    "        eval_dataloader, \n",
    "        desc=f\"Epoch {epoch+1}/{num_epochs} - Evaluating\",\n",
    "        unit=\"batch\",\n",
    "        ncols=bar_length,\n",
    "        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'\n",
    "    )\n",
    "    \n",
    "    for batch in eval_progress_bar:\n",
    "        with torch.no_grad():\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        batch_preds = torch.argmax(logits.detach().cpu(), dim=1).numpy()\n",
    "        batch_labels = torch.argmax(inputs[\"labels\"].detach().cpu(), dim=1).numpy()\n",
    "        \n",
    "        preds.extend(batch_preds)\n",
    "        out_label_ids.extend(batch_labels)\n",
    "        \n",
    "        # Update progress bar with current batch stats\n",
    "        if len(preds) > 0:\n",
    "            current_accuracy = accuracy_score(out_label_ids, preds)\n",
    "            eval_progress_bar.set_postfix({'acc': f'{current_accuracy:.3f}'})\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    accuracy = accuracy_score(out_label_ids, preds)\n",
    "    f1 = f1_score(out_label_ids, preds, average='weighted')\n",
    "    recall = recall_score(out_label_ids, preds, average='weighted')\n",
    "    precision = precision_score(out_label_ids, preds, average='weighted')\n",
    "    \n",
    "    # Update the training progress bar with final metrics\n",
    "    train_progress_bar.set_postfix({\n",
    "        'loss': f'{avg_loss:.4f}',\n",
    "        'acc': f'{accuracy:.3f}',\n",
    "        'f1': f'{f1:.3f}',\n",
    "        'recall': f'{recall:.3f}',\n",
    "        'precision': f'{precision:.3f}'\n",
    "    })\n",
    "    \n",
    "    # Close progress bars\n",
    "    train_progress_bar.close()\n",
    "    eval_progress_bar.close()\n",
    "    \n",
    "    # Optional: Print summary at the end of each epoch\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} Summary:\")\n",
    "    print(f\"Train Loss: {avg_loss:.4f} | Accuracy: {accuracy:.4f} | F1: {f1:.4f} | Recall: {recall:.4f} | Precision: {precision:.4f}\")\n",
    "    \n",
    "    # Memory check during training\n",
    "    if i % 10 == 0:\n",
    "        print_cuda_memory()\n",
    "    \n",
    "    # Clean up after epoch\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f28602b",
   "metadata": {
    "id": "fDte1szszhiK"
   },
   "source": [
    "# 6. Inference Pipeline\n",
    "1. `tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "`: You need use the same tokenizer that was use for fine-tunning\n",
    "2. `logits.detach().cpu()`\n",
    "  * `detach is done to prevent  unintentional back-propogation\n",
    "  * `.cpu` is done so that the output is compatible with scikit-learn libraries for further computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0eaa69d",
   "metadata": {
    "id": "RKwJIhUTkja9"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def inference(text, model,  label, device='cuda'):\n",
    "    # Load the tokenizer\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    # Move input tensors to the specified device (default: 'cpu')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Set the model to evaluation mode and perform inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Get the index of the predicted label\n",
    "    pred_label_idx = torch.argmax(logits.detach().cpu(), dim=1).item()\n",
    "\n",
    "    print(f\"Predicted label index: {pred_label_idx}, actual label {label}\")\n",
    "    return pred_label_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d3130b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1D8y9AKRxfSS",
    "outputId": "40adf168-5d35-471d-da8a-aee57a1fb852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label index: 1, actual label 1.0\n",
      "Predicted label index: 0, actual label 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://abcnews.go.com/US/tornado-confirmed-delaware-powerful-storm-moves-east/story?id=98293454\n",
    "text='\\\n",
    "WASHINGTON (ABC) A confirmed tornado was located near Bridgeville in Sussex County, Delaware, shortly after 6 p.m. ET Saturday, moving east at 50 mph, according to the National Weather Service. Downed trees and wires were reported in the area.\\\n",
    "'\n",
    "inference(text, model, 1.0)\n",
    "text=\"this is definately junk text I am typing\"\n",
    "inference(text, model, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d73a3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory cleared\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ab6416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
